{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "from collections import namedtuple\n",
    "import numpy as np           # Handle matrices\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from collections import deque# Ordered collection with ends\n",
    "from keras.models import Sequential\n",
    "from keras.layers import * #or use import Dense, Activation, Flatten\n",
    "from keras.optimizers import * # or use import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "env_name = None # we're going to us the unity editor as the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Environment E\n",
    "Initialize replay Memory M with capacity N (= finite capacity)\n",
    "Initialize the DQN weights w\n",
    "for episode in max_episode:\n",
    "    s = Environment state\n",
    "    for steps in max_steps:\n",
    "         Choose action a from state s using epsilon greedy.\n",
    "         Take action a, get r (reward) and s' (next state)\n",
    "         Store experience tuple <s, a, r, s'> in M\n",
    "         s = s' (state = new_state)\n",
    "         \n",
    "         Get random minibatch of exp tuples from M\n",
    "         Set Q_target = reward(s,a) +  γmaxQ(s')\n",
    "         Update w =  α(Q_target - Q_value) *  ∇w Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hl_nodes, input_dim=input_dimension, activation = 'relu'))\n",
    "    model.add(Dense(output_dimension, activation = 'linear'))\n",
    "    model.set_weights(weights)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_observation = env.reset()\n",
    "VALID_ACTIONS = [1:20]\n",
    "memory = deque(maxlen=1000)\n",
    "input_dimension = 129\n",
    "output_dimension = 20\n",
    "hl_nodes = \n",
    "weights = \n",
    "q_nn = Build_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy(state_prev, env)\n",
    "obs, reward, done, info = env.step(action)\n",
    "state_next = discretize_state(obs, s_bounds, n_s)\n",
    "#add the state_prev, action, reward, state_new, done to memory\n",
    "memory.append([state_prev, action, reward, state_next,done])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array(x[0] for x in memory)\n",
    "states_next = np.array([np.zeros(input_dimensions) if x[4] else x[3] for x in memory])\n",
    "q_values = q_nn.predict(states)\n",
    "q_values_next = q_nn.predict(states_next)\n",
    "\n",
    "for i in range(len(memory)):\n",
    "    state_prev, action, reward, state_next, done = memory[i]\n",
    "    if done:\n",
    "        q_values[i, action] = reward\n",
    "    else:\n",
    "        best_q = np.amax(q_values_next[i])\n",
    "        bellman_q = reward + discount_rate * best_q\n",
    "        q_values[i, action] = bellman_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_nn.fit(states, q_values, epoch=1, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
